{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdl6h8A1NsdDyc7WNKpTnM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinisrferreira/Pseudodiagnosticity-in-a-continuous-learning-environment-Reis-2020-/blob/main/Reis_Pseudodiagnosticity_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIZ01hlwA3CR",
        "outputId": "e7e24144-76d7-4606-91c4-ca3977dcd781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from Google Drive\n",
        "folder_path = '/content/drive/My Drive/ProjetoGulbenkian/'\n",
        "wicked_train_set = pd.read_csv(f'{folder_path}wicked_train_set.csv')\n",
        "wicked_test_set = pd.read_csv(f'{folder_path}wicked_test_set.csv')\n",
        "kind_train_set = pd.read_csv(f'{folder_path}kind_train_set.csv')\n",
        "kind_test_set = pd.read_csv(f'{folder_path}kind_test_set.csv')\n",
        "#universal_kind_train_set = pd.read_csv(f'{folder_path}universal_kind_train_set.csv')\n",
        "#universal_kind_test_set = pd.read_csv(f'{folder_path}universal_kind_test_set.csv')"
      ],
      "metadata": {
        "id": "-e63kiw3BFOe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust labels to fit CrossEntropyLoss requirements\n",
        "def adjust_labels(df):\n",
        "    df['label'] = df['label'].map({-1: 1, 0: 0, 1: 2})\n",
        "    return df\n",
        "\n",
        "wicked_train_set = adjust_labels(wicked_train_set)\n",
        "wicked_test_set = adjust_labels(wicked_test_set)\n",
        "kind_train_set = adjust_labels(kind_train_set)\n",
        "kind_test_set = adjust_labels(kind_test_set)\n",
        "#universal_kind_train_set = adjust_labels(universal_kind_train_set)\n",
        "#universal_kind_test_set = adjust_labels(universal_kind_test_set)\n"
      ],
      "metadata": {
        "id": "LIJzch3UH9iL"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data function\n",
        "def prepare_data(df):\n",
        "    X = df['input'].apply(eval).values.tolist()  # Convert string representation of list to actual list\n",
        "    y = df['label'].values\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.long)\n",
        "    return TensorDataset(X_tensor, y_tensor)"
      ],
      "metadata": {
        "id": "kPLLXpfnE8sE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiagnosticNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiagnosticNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 10)\n",
        "        self.fc2 = nn.Linear(10, 3)  # 3 classes for the 3 strategies\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # Softmax will be applied in the CrossEntropyLoss\n",
        "        return x"
      ],
      "metadata": {
        "id": "Q5qcT2e6BSq2"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_wicked = prepare_data(wicked_train_set)\n",
        "test_data_wicked = prepare_data(wicked_test_set)\n",
        "train_data_kind = prepare_data(kind_train_set)\n",
        "test_data_kind = prepare_data(kind_test_set)\n",
        "#train_data_universal_kind = prepare_data(universal_kind_train_set)\n",
        "#test_data_universal_kind = prepare_data(universal_kind_test_set)"
      ],
      "metadata": {
        "id": "OL7C6QzKBjNj"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_loader_wicked = DataLoader(train_data_wicked, batch_size=batch_size, shuffle=True)\n",
        "test_loader_wicked = DataLoader(test_data_wicked, batch_size=batch_size, shuffle=False)\n",
        "train_loader_kind = DataLoader(train_data_kind, batch_size=batch_size, shuffle=True)\n",
        "test_loader_kind = DataLoader(test_data_kind, batch_size=batch_size, shuffle=False)\n",
        "#train_loader_universal_kind = DataLoader(train_data_universal_kind, batch_size=batch_size, shuffle=True)\n",
        "#test_loader_universal_kind = DataLoader(test_data_universal_kind, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "yS-C8kr9BqcF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=50):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "ktOZqL0gBzvq"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Function\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "xECH4A8OCT1a"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, criterion, and optimizer\n",
        "model_wicked = DiagnosticNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_wicked.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "KoNCoQpKFobt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model_wicked, train_loader_wicked, criterion, optimizer, num_epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8-i8InJGFqe",
        "outputId": "9e118641-fa6e-400d-b99c-414b76d89ef4"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 1.0855\n",
            "Epoch [2/50], Loss: 1.0508\n",
            "Epoch [3/50], Loss: 1.0161\n",
            "Epoch [4/50], Loss: 0.9826\n",
            "Epoch [5/50], Loss: 0.9499\n",
            "Epoch [6/50], Loss: 0.9184\n",
            "Epoch [7/50], Loss: 0.8889\n",
            "Epoch [8/50], Loss: 0.8612\n",
            "Epoch [9/50], Loss: 0.8363\n",
            "Epoch [10/50], Loss: 0.8140\n",
            "Epoch [11/50], Loss: 0.7957\n",
            "Epoch [12/50], Loss: 0.7799\n",
            "Epoch [13/50], Loss: 0.7664\n",
            "Epoch [14/50], Loss: 0.7546\n",
            "Epoch [15/50], Loss: 0.7445\n",
            "Epoch [16/50], Loss: 0.7355\n",
            "Epoch [17/50], Loss: 0.7273\n",
            "Epoch [18/50], Loss: 0.7200\n",
            "Epoch [19/50], Loss: 0.7132\n",
            "Epoch [20/50], Loss: 0.7076\n",
            "Epoch [21/50], Loss: 0.7017\n",
            "Epoch [22/50], Loss: 0.6968\n",
            "Epoch [23/50], Loss: 0.6928\n",
            "Epoch [24/50], Loss: 0.6880\n",
            "Epoch [25/50], Loss: 0.6839\n",
            "Epoch [26/50], Loss: 0.6803\n",
            "Epoch [27/50], Loss: 0.6768\n",
            "Epoch [28/50], Loss: 0.6736\n",
            "Epoch [29/50], Loss: 0.6704\n",
            "Epoch [30/50], Loss: 0.6678\n",
            "Epoch [31/50], Loss: 0.6651\n",
            "Epoch [32/50], Loss: 0.6626\n",
            "Epoch [33/50], Loss: 0.6604\n",
            "Epoch [34/50], Loss: 0.6582\n",
            "Epoch [35/50], Loss: 0.6561\n",
            "Epoch [36/50], Loss: 0.6543\n",
            "Epoch [37/50], Loss: 0.6524\n",
            "Epoch [38/50], Loss: 0.6506\n",
            "Epoch [39/50], Loss: 0.6493\n",
            "Epoch [40/50], Loss: 0.6477\n",
            "Epoch [41/50], Loss: 0.6461\n",
            "Epoch [42/50], Loss: 0.6446\n",
            "Epoch [43/50], Loss: 0.6430\n",
            "Epoch [44/50], Loss: 0.6418\n",
            "Epoch [45/50], Loss: 0.6404\n",
            "Epoch [46/50], Loss: 0.6395\n",
            "Epoch [47/50], Loss: 0.6378\n",
            "Epoch [48/50], Loss: 0.6366\n",
            "Epoch [49/50], Loss: 0.6354\n",
            "Epoch [50/50], Loss: 0.6341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model, criterion, and optimizer\n",
        "model_wicked = DiagnosticNet()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_wicked.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "train_model(model_wicked, train_loader_wicked, criterion, optimizer, num_epochs=150)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model_wicked, test_loader_wicked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LMBZZ8dCX7s",
        "outputId": "6727ef81-1eb6-4179-8469-f3532cc7b246"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/150], Loss: 1.0911\n",
            "Epoch [2/150], Loss: 1.0506\n",
            "Epoch [3/150], Loss: 1.0105\n",
            "Epoch [4/150], Loss: 0.9705\n",
            "Epoch [5/150], Loss: 0.9310\n",
            "Epoch [6/150], Loss: 0.8935\n",
            "Epoch [7/150], Loss: 0.8578\n",
            "Epoch [8/150], Loss: 0.8263\n",
            "Epoch [9/150], Loss: 0.7990\n",
            "Epoch [10/150], Loss: 0.7760\n",
            "Epoch [11/150], Loss: 0.7555\n",
            "Epoch [12/150], Loss: 0.7387\n",
            "Epoch [13/150], Loss: 0.7232\n",
            "Epoch [14/150], Loss: 0.7111\n",
            "Epoch [15/150], Loss: 0.7006\n",
            "Epoch [16/150], Loss: 0.6926\n",
            "Epoch [17/150], Loss: 0.6846\n",
            "Epoch [18/150], Loss: 0.6779\n",
            "Epoch [19/150], Loss: 0.6725\n",
            "Epoch [20/150], Loss: 0.6670\n",
            "Epoch [21/150], Loss: 0.6625\n",
            "Epoch [22/150], Loss: 0.6585\n",
            "Epoch [23/150], Loss: 0.6551\n",
            "Epoch [24/150], Loss: 0.6515\n",
            "Epoch [25/150], Loss: 0.6489\n",
            "Epoch [26/150], Loss: 0.6459\n",
            "Epoch [27/150], Loss: 0.6437\n",
            "Epoch [28/150], Loss: 0.6418\n",
            "Epoch [29/150], Loss: 0.6391\n",
            "Epoch [30/150], Loss: 0.6367\n",
            "Epoch [31/150], Loss: 0.6352\n",
            "Epoch [32/150], Loss: 0.6327\n",
            "Epoch [33/150], Loss: 0.6308\n",
            "Epoch [34/150], Loss: 0.6290\n",
            "Epoch [35/150], Loss: 0.6271\n",
            "Epoch [36/150], Loss: 0.6254\n",
            "Epoch [37/150], Loss: 0.6236\n",
            "Epoch [38/150], Loss: 0.6221\n",
            "Epoch [39/150], Loss: 0.6203\n",
            "Epoch [40/150], Loss: 0.6185\n",
            "Epoch [41/150], Loss: 0.6165\n",
            "Epoch [42/150], Loss: 0.6148\n",
            "Epoch [43/150], Loss: 0.6130\n",
            "Epoch [44/150], Loss: 0.6111\n",
            "Epoch [45/150], Loss: 0.6097\n",
            "Epoch [46/150], Loss: 0.6073\n",
            "Epoch [47/150], Loss: 0.6058\n",
            "Epoch [48/150], Loss: 0.6038\n",
            "Epoch [49/150], Loss: 0.6020\n",
            "Epoch [50/150], Loss: 0.6000\n",
            "Epoch [51/150], Loss: 0.5980\n",
            "Epoch [52/150], Loss: 0.5961\n",
            "Epoch [53/150], Loss: 0.5943\n",
            "Epoch [54/150], Loss: 0.5925\n",
            "Epoch [55/150], Loss: 0.5904\n",
            "Epoch [56/150], Loss: 0.5882\n",
            "Epoch [57/150], Loss: 0.5863\n",
            "Epoch [58/150], Loss: 0.5842\n",
            "Epoch [59/150], Loss: 0.5828\n",
            "Epoch [60/150], Loss: 0.5800\n",
            "Epoch [61/150], Loss: 0.5780\n",
            "Epoch [62/150], Loss: 0.5757\n",
            "Epoch [63/150], Loss: 0.5737\n",
            "Epoch [64/150], Loss: 0.5717\n",
            "Epoch [65/150], Loss: 0.5693\n",
            "Epoch [66/150], Loss: 0.5670\n",
            "Epoch [67/150], Loss: 0.5649\n",
            "Epoch [68/150], Loss: 0.5625\n",
            "Epoch [69/150], Loss: 0.5604\n",
            "Epoch [70/150], Loss: 0.5581\n",
            "Epoch [71/150], Loss: 0.5556\n",
            "Epoch [72/150], Loss: 0.5534\n",
            "Epoch [73/150], Loss: 0.5512\n",
            "Epoch [74/150], Loss: 0.5487\n",
            "Epoch [75/150], Loss: 0.5465\n",
            "Epoch [76/150], Loss: 0.5438\n",
            "Epoch [77/150], Loss: 0.5414\n",
            "Epoch [78/150], Loss: 0.5392\n",
            "Epoch [79/150], Loss: 0.5368\n",
            "Epoch [80/150], Loss: 0.5340\n",
            "Epoch [81/150], Loss: 0.5316\n",
            "Epoch [82/150], Loss: 0.5291\n",
            "Epoch [83/150], Loss: 0.5268\n",
            "Epoch [84/150], Loss: 0.5242\n",
            "Epoch [85/150], Loss: 0.5224\n",
            "Epoch [86/150], Loss: 0.5194\n",
            "Epoch [87/150], Loss: 0.5167\n",
            "Epoch [88/150], Loss: 0.5152\n",
            "Epoch [89/150], Loss: 0.5119\n",
            "Epoch [90/150], Loss: 0.5093\n",
            "Epoch [91/150], Loss: 0.5069\n",
            "Epoch [92/150], Loss: 0.5043\n",
            "Epoch [93/150], Loss: 0.5019\n",
            "Epoch [94/150], Loss: 0.4993\n",
            "Epoch [95/150], Loss: 0.4968\n",
            "Epoch [96/150], Loss: 0.4950\n",
            "Epoch [97/150], Loss: 0.4918\n",
            "Epoch [98/150], Loss: 0.4896\n",
            "Epoch [99/150], Loss: 0.4872\n",
            "Epoch [100/150], Loss: 0.4844\n",
            "Epoch [101/150], Loss: 0.4819\n",
            "Epoch [102/150], Loss: 0.4797\n",
            "Epoch [103/150], Loss: 0.4770\n",
            "Epoch [104/150], Loss: 0.4746\n",
            "Epoch [105/150], Loss: 0.4720\n",
            "Epoch [106/150], Loss: 0.4696\n",
            "Epoch [107/150], Loss: 0.4674\n",
            "Epoch [108/150], Loss: 0.4653\n",
            "Epoch [109/150], Loss: 0.4625\n",
            "Epoch [110/150], Loss: 0.4604\n",
            "Epoch [111/150], Loss: 0.4577\n",
            "Epoch [112/150], Loss: 0.4554\n",
            "Epoch [113/150], Loss: 0.4531\n",
            "Epoch [114/150], Loss: 0.4512\n",
            "Epoch [115/150], Loss: 0.4487\n",
            "Epoch [116/150], Loss: 0.4464\n",
            "Epoch [117/150], Loss: 0.4443\n",
            "Epoch [118/150], Loss: 0.4418\n",
            "Epoch [119/150], Loss: 0.4397\n",
            "Epoch [120/150], Loss: 0.4376\n",
            "Epoch [121/150], Loss: 0.4354\n",
            "Epoch [122/150], Loss: 0.4328\n",
            "Epoch [123/150], Loss: 0.4306\n",
            "Epoch [124/150], Loss: 0.4284\n",
            "Epoch [125/150], Loss: 0.4263\n",
            "Epoch [126/150], Loss: 0.4242\n",
            "Epoch [127/150], Loss: 0.4221\n",
            "Epoch [128/150], Loss: 0.4196\n",
            "Epoch [129/150], Loss: 0.4178\n",
            "Epoch [130/150], Loss: 0.4157\n",
            "Epoch [131/150], Loss: 0.4136\n",
            "Epoch [132/150], Loss: 0.4112\n",
            "Epoch [133/150], Loss: 0.4092\n",
            "Epoch [134/150], Loss: 0.4069\n",
            "Epoch [135/150], Loss: 0.4047\n",
            "Epoch [136/150], Loss: 0.4032\n",
            "Epoch [137/150], Loss: 0.4003\n",
            "Epoch [138/150], Loss: 0.3977\n",
            "Epoch [139/150], Loss: 0.3955\n",
            "Epoch [140/150], Loss: 0.3931\n",
            "Epoch [141/150], Loss: 0.3903\n",
            "Epoch [142/150], Loss: 0.3875\n",
            "Epoch [143/150], Loss: 0.3844\n",
            "Epoch [144/150], Loss: 0.3814\n",
            "Epoch [145/150], Loss: 0.3786\n",
            "Epoch [146/150], Loss: 0.3757\n",
            "Epoch [147/150], Loss: 0.3729\n",
            "Epoch [148/150], Loss: 0.3702\n",
            "Epoch [149/150], Loss: 0.3675\n",
            "Epoch [150/150], Loss: 0.3648\n",
            "Accuracy: 85.50%\n"
          ]
        }
      ]
    }
  ]
}